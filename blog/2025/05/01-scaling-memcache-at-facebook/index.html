<!DOCTYPE html><html lang="en" data-astro-cid-bvzihdzo> <head><!-- Global Metadata --><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1"><link rel="icon" type="image/svg+xml" href="/favicon.svg"><link rel="sitemap" href="/sitemap-index.xml"><link rel="alternate" type="application/rss+xml" title="BigJoe" href="https://www.joewalker.xzy/rss.xml"><meta name="generator" content="Astro v5.7.4"><!-- Font preloads --><link rel="preload" href="/fonts/atkinson-regular.woff" as="font" type="font/woff" crossorigin><link rel="preload" href="/fonts/atkinson-bold.woff" as="font" type="font/woff" crossorigin><!-- Canonical URL --><link rel="canonical" href="https://www.joewalker.xzy/blog/2025/05/01-scaling-memcache-at-facebook/"><!-- Primary Meta Tags --><title>Facebook รองรับ Request Billion/Sec โดยใช้ Memcached ได้อย่างไร?</title><meta name="title" content="Facebook รองรับ Request Billion/Sec โดยใช้ Memcached ได้อย่างไร?"><meta name="description" content="Facebook รองรับ Request Billion/Sec โดยใช้ Memcached ได้อย่างไร?"><!-- Open Graph / Facebook --><meta property="og:type" content="website"><meta property="og:url" content="https://www.joewalker.xzy/blog/2025/05/01-scaling-memcache-at-facebook/"><meta property="og:title" content="Facebook รองรับ Request Billion/Sec โดยใช้ Memcached ได้อย่างไร?"><meta property="og:description" content="Facebook รองรับ Request Billion/Sec โดยใช้ Memcached ได้อย่างไร?"><meta property="og:image" content="https://www.joewalker.xzy/blog-placeholder-1.jpg"><!-- Twitter --><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://www.joewalker.xzy/blog/2025/05/01-scaling-memcache-at-facebook/"><meta property="twitter:title" content="Facebook รองรับ Request Billion/Sec โดยใช้ Memcached ได้อย่างไร?"><meta property="twitter:description" content="Facebook รองรับ Request Billion/Sec โดยใช้ Memcached ได้อย่างไร?"><meta property="twitter:image" content="https://www.joewalker.xzy/blog-placeholder-1.jpg"><!-- Google Analytic --><script src="https://www.googletagmanager.com/gtag/js?id=G-72RJB9H4WY"></script><script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-72RJB9H4WY');
</script><script type="module" src="/_astro/BaseHead.astro_astro_type_script_index_0_lang.BiByk9QU.js"></script><style>/*! tailwindcss v4.1.4 | MIT License | https://tailwindcss.com */@layer properties{@supports (((-webkit-hyphens:none)) and (not (margin-trim:inline))) or ((-moz-orient:inline) and (not (color:rgb(from red r g b)))){*,:before,:after,::backdrop{--tw-rotate-x:initial;--tw-rotate-y:initial;--tw-rotate-z:initial;--tw-skew-x:initial;--tw-skew-y:initial;--tw-space-y-reverse:0;--tw-border-style:solid;--tw-leading:initial;--tw-font-weight:initial;--tw-shadow:0 0 #0000;--tw-shadow-color:initial;--tw-shadow-alpha:100%;--tw-inset-shadow:0 0 #0000;--tw-inset-shadow-color:initial;--tw-inset-shadow-alpha:100%;--tw-ring-color:initial;--tw-ring-shadow:0 0 #0000;--tw-inset-ring-color:initial;--tw-inset-ring-shadow:0 0 #0000;--tw-ring-inset:initial;--tw-ring-offset-width:0px;--tw-ring-offset-color:#fff;--tw-ring-offset-shadow:0 0 #0000;--tw-outline-style:solid}}}@layer theme{:root,:host{--font-sans:ui-sans-serif,system-ui,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";--font-mono:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace;--color-blue-600:oklch(54.6% .245 262.881);--color-gray-200:oklch(92.8% .006 264.531);--color-gray-600:oklch(44.6% .03 256.802);--color-gray-700:oklch(37.3% .034 259.733);--color-white:#fff;--spacing:.25rem;--container-xs:20rem;--container-sm:24rem;--text-lg:1.125rem;--text-lg--line-height:calc(1.75/1.125);--text-2xl:1.5rem;--text-2xl--line-height:calc(2/1.5);--font-weight-medium:500;--font-weight-bold:700;--leading-relaxed:1.625;--radius-md:.375rem;--default-transition-duration:.15s;--default-transition-timing-function:cubic-bezier(.4,0,.2,1);--default-font-family:var(--font-sans);--default-mono-font-family:var(--font-mono)}}@layer base{*,:after,:before,::backdrop{box-sizing:border-box;border:0 solid;margin:0;padding:0}::file-selector-button{box-sizing:border-box;border:0 solid;margin:0;padding:0}html,:host{-webkit-text-size-adjust:100%;tab-size:4;line-height:1.5;font-family:var(--default-font-family,ui-sans-serif,system-ui,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji");font-feature-settings:var(--default-font-feature-settings,normal);font-variation-settings:var(--default-font-variation-settings,normal);-webkit-tap-highlight-color:transparent}hr{height:0;color:inherit;border-top-width:1px}abbr:where([title]){-webkit-text-decoration:underline dotted;text-decoration:underline dotted}h1,h2,h3,h4,h5,h6{font-size:inherit;font-weight:inherit}a{color:inherit;-webkit-text-decoration:inherit;text-decoration:inherit}b,strong{font-weight:bolder}code,kbd,samp,pre{font-family:var(--default-mono-font-family,ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace);font-feature-settings:var(--default-mono-font-feature-settings,normal);font-variation-settings:var(--default-mono-font-variation-settings,normal);font-size:1em}small{font-size:80%}sub,sup{vertical-align:baseline;font-size:75%;line-height:0;position:relative}sub{bottom:-.25em}sup{top:-.5em}table{text-indent:0;border-color:inherit;border-collapse:collapse}:-moz-focusring{outline:auto}progress{vertical-align:baseline}summary{display:list-item}ol,ul,menu{list-style:none}img,svg,video,canvas,audio,iframe,embed,object{vertical-align:middle;display:block}img,video{max-width:100%;height:auto}button,input,select,optgroup,textarea{font:inherit;font-feature-settings:inherit;font-variation-settings:inherit;letter-spacing:inherit;color:inherit;opacity:1;background-color:#0000;border-radius:0}::file-selector-button{font:inherit;font-feature-settings:inherit;font-variation-settings:inherit;letter-spacing:inherit;color:inherit;opacity:1;background-color:#0000;border-radius:0}:where(select:is([multiple],[size])) optgroup{font-weight:bolder}:where(select:is([multiple],[size])) optgroup option{padding-inline-start:20px}::file-selector-button{margin-inline-end:4px}::placeholder{opacity:1}@supports (not ((-webkit-appearance:-apple-pay-button))) or (contain-intrinsic-size:1px){::placeholder{color:currentColor}@supports (color:color-mix(in lab,red,red)){::placeholder{color:color-mix(in oklab,currentcolor 50%,transparent)}}}textarea{resize:vertical}::-webkit-search-decoration{-webkit-appearance:none}::-webkit-date-and-time-value{min-height:1lh;text-align:inherit}::-webkit-datetime-edit{display:inline-flex}::-webkit-datetime-edit-fields-wrapper{padding:0}::-webkit-datetime-edit{padding-block:0}::-webkit-datetime-edit-year-field{padding-block:0}::-webkit-datetime-edit-month-field{padding-block:0}::-webkit-datetime-edit-day-field{padding-block:0}::-webkit-datetime-edit-hour-field{padding-block:0}::-webkit-datetime-edit-minute-field{padding-block:0}::-webkit-datetime-edit-second-field{padding-block:0}::-webkit-datetime-edit-millisecond-field{padding-block:0}::-webkit-datetime-edit-meridiem-field{padding-block:0}:-moz-ui-invalid{box-shadow:none}button,input:where([type=button],[type=reset],[type=submit]){appearance:button}::file-selector-button{appearance:button}::-webkit-inner-spin-button{height:auto}::-webkit-outer-spin-button{height:auto}[hidden]:where(:not([hidden=until-found])){display:none!important}}@layer components;@layer utilities{.invisible{visibility:hidden}.absolute{position:absolute}.fixed{position:fixed}.relative{position:relative}.static{position:static}.inset-0{inset:calc(var(--spacing)*0)}.isolate{isolation:isolate}.container{width:100%}@media (min-width:40rem){.container{max-width:40rem}}@media (min-width:48rem){.container{max-width:48rem}}@media (min-width:64rem){.container{max-width:64rem}}@media (min-width:80rem){.container{max-width:80rem}}@media (min-width:96rem){.container{max-width:96rem}}.m-2{margin:calc(var(--spacing)*2)}.m-26{margin:calc(var(--spacing)*26)}.m-30{margin:calc(var(--spacing)*30)}.m-42{margin:calc(var(--spacing)*42)}.m-50{margin:calc(var(--spacing)*50)}.m-51{margin:calc(var(--spacing)*51)}.m-70{margin:calc(var(--spacing)*70)}.m-76{margin:calc(var(--spacing)*76)}.m-77{margin:calc(var(--spacing)*77)}.m-88{margin:calc(var(--spacing)*88)}.m-94{margin:calc(var(--spacing)*94)}.m-138{margin:calc(var(--spacing)*138)}.m-148{margin:calc(var(--spacing)*148)}.m-155{margin:calc(var(--spacing)*155)}.m-195{margin:calc(var(--spacing)*195)}.m-260{margin:calc(var(--spacing)*260)}.m-307{margin:calc(var(--spacing)*307)}.m-1886{margin:calc(var(--spacing)*1886)}.m-2029{margin:calc(var(--spacing)*2029)}.m-2530{margin:calc(var(--spacing)*2530)}.m-2869{margin:calc(var(--spacing)*2869)}.mt-1{margin-top:calc(var(--spacing)*1)}.mt-8{margin-top:calc(var(--spacing)*8)}.mb-2{margin-bottom:calc(var(--spacing)*2)}.block{display:block}.contents{display:contents}.flex{display:flex}.grid{display:grid}.inline-block{display:inline-block}.table{display:table}.h-4{height:calc(var(--spacing)*4)}.h-64{height:calc(var(--spacing)*64)}.min-h-screen{min-height:100vh}.w-4{width:calc(var(--spacing)*4)}.w-full{width:100%}.max-w-sm{max-width:var(--container-sm)}.flex-1{flex:1}.flex-grow{flex-grow:1}.transform{transform:var(--tw-rotate-x,)var(--tw-rotate-y,)var(--tw-rotate-z,)var(--tw-skew-x,)var(--tw-skew-y,)}.resize{resize:both}.grid-cols-1{grid-template-columns:repeat(1,minmax(0,1fr))}.grid-cols-2{grid-template-columns:repeat(2,minmax(0,1fr))}.flex-col{flex-direction:column}.flex-wrap{flex-wrap:wrap}.items-center{align-items:center}.justify-between{justify-content:space-between}.justify-end{justify-content:flex-end}.justify-items-center{justify-items:center}.gap-4{gap:calc(var(--spacing)*4)}.gap-8{gap:calc(var(--spacing)*8)}:where(.space-y-1>:not(:last-child)){--tw-space-y-reverse:0;margin-block-start:calc(calc(var(--spacing)*1)*var(--tw-space-y-reverse));margin-block-end:calc(calc(var(--spacing)*1)*calc(1 - var(--tw-space-y-reverse)))}.overflow-hidden{overflow:hidden}.rounded{border-radius:.25rem}.rounded-md{border-radius:var(--radius-md)}.border{border-style:var(--tw-border-style);border-width:1px}.border-t{border-top-style:var(--tw-border-style);border-top-width:1px}.border-b{border-bottom-style:var(--tw-border-style);border-bottom-width:1px}.border-gray-200{border-color:var(--color-gray-200)}.bg-gray-200{background-color:var(--color-gray-200)}.object-cover{object-fit:cover}.p-3{padding:calc(var(--spacing)*3)}.p-4{padding:calc(var(--spacing)*4)}.p-6{padding:calc(var(--spacing)*6)}.px-3{padding-inline:calc(var(--spacing)*3)}.py-1{padding-block:calc(var(--spacing)*1)}.py-4{padding-block:calc(var(--spacing)*4)}.py-6{padding-block:calc(var(--spacing)*6)}.pl-6{padding-left:calc(var(--spacing)*6)}.text-left{text-align:left}.text-right{text-align:right}.text-2xl{font-size:var(--text-2xl);line-height:var(--tw-leading,var(--text-2xl--line-height))}.text-lg{font-size:var(--text-lg);line-height:var(--tw-leading,var(--text-lg--line-height))}.leading-relaxed{--tw-leading:var(--leading-relaxed);line-height:var(--leading-relaxed)}.font-bold{--tw-font-weight:var(--font-weight-bold);font-weight:var(--font-weight-bold)}.font-medium{--tw-font-weight:var(--font-weight-medium);font-weight:var(--font-weight-medium)}.text-blue-600{color:var(--color-blue-600)}.text-gray-600{color:var(--color-gray-600)}.text-gray-700{color:var(--color-gray-700)}.text-white{color:var(--color-white)}.shadow-lg{--tw-shadow:0 10px 15px -3px var(--tw-shadow-color,#0000001a),0 4px 6px -4px var(--tw-shadow-color,#0000001a);box-shadow:var(--tw-inset-shadow),var(--tw-inset-ring-shadow),var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-shadow)}.outline{outline-style:var(--tw-outline-style);outline-width:1px}.transition{transition-property:color,background-color,border-color,outline-color,text-decoration-color,fill,stroke,--tw-gradient-from,--tw-gradient-via,--tw-gradient-to,opacity,box-shadow,transform,translate,scale,rotate,filter,-webkit-backdrop-filter,backdrop-filter;transition-timing-function:var(--tw-ease,var(--default-transition-timing-function));transition-duration:var(--tw-duration,var(--default-transition-duration))}.transition-colors{transition-property:color,background-color,border-color,outline-color,text-decoration-color,fill,stroke,--tw-gradient-from,--tw-gradient-via,--tw-gradient-to;transition-timing-function:var(--tw-ease,var(--default-transition-timing-function));transition-duration:var(--tw-duration,var(--default-transition-duration))}.transition-transform{transition-property:transform,translate,scale,rotate;transition-timing-function:var(--tw-ease,var(--default-transition-timing-function));transition-duration:var(--tw-duration,var(--default-transition-duration))}@media (hover:hover){.hover\:border-blue-600:hover{border-color:var(--color-blue-600)}.hover\:bg-gray-200:hover{background-color:var(--color-gray-200)}}@media (min-width:48rem){.md\:w-xs{width:var(--container-xs)}.md\:grid-cols-2{grid-template-columns:repeat(2,minmax(0,1fr))}.md\:flex-row{flex-direction:row}.md\:border-r-2{border-right-style:var(--tw-border-style);border-right-width:2px}.md\:border-b-0{border-bottom-style:var(--tw-border-style);border-bottom-width:0}}@media (min-width:64rem){.lg\:grid-cols-3{grid-template-columns:repeat(3,minmax(0,1fr))}}}:root{--accent:#2337ff;--accent-dark:#000d8a;--black:15,18,25;--gray:96,115,159;--gray-light:229,233,240;--gray-dark:34,41,57;--gray-gradient:rgba(var(--gray-light),50%),#fff;--green-50:#f0fdf4;--green-100:#dcfce7;--green-200:#bbf7d0;--blue-50:#eff6ff;--blue-100:#dbeafe;--blue-200:#bfdbfe;--box-shadow:0 2px 6px rgba(var(--gray),25%),0 8px 24px rgba(var(--gray),33%),0 16px 32px rgba(var(--gray),33%)}@font-face{font-family:Atkinson;src:url(/fonts/atkinson-regular.woff)format("woff");font-weight:400;font-style:normal;font-display:swap}@font-face{font-family:Atkinson;src:url(/fonts/atkinson-bold.woff)format("woff");font-weight:700;font-style:normal;font-display:swap}body{text-align:left;background:linear-gradient(var(--gray-gradient))no-repeat;word-wrap:break-word;overflow-wrap:break-word;color:rgb(var(--gray-dark));background-size:100% 600px;margin:0;padding:0;font-family:Atkinson,sans-serif;font-size:20px;line-height:1.7}main{width:720px;max-width:calc(100% - 2em);margin:auto;padding:3em 1em}h1,h2,h3,h4,h5,h6{color:rgb(var(--black));margin:0 0 .5rem;line-height:1.2}h1{font-size:3.052em}h2{font-size:2.441em}h3{font-size:1.953em}h4{font-size:1.563em}h5{font-size:1.25em}strong,b{font-weight:700}a,a:hover{color:var(--accent)}p{margin-bottom:1em}p:has(+ul),p:has(+pre),ul li p{margin-bottom:0}ol:has(+blockquote),ul:has(+blockquote),.prose ol:has(+p),.prose ul:has(+p){margin-bottom:1em}.prose p:has(+ol,ul){margin-bottom:0}.prose ol,ul{padding-left:1.5em;list-style-type:disc}.prose ul ul{padding-left:1.5em;list-style-type:circle}textarea{width:100%;font-size:16px}input{font-size:16px}table{width:100%;margin-bottom:1em}table thead tr{background-color:rgba(var(--gray-light),50%)}table thead tr th{border-radius:3px;padding:.5rem}table tbody tr:not(:last-child){border-bottom:1px solid rgba(var(--gray-light),80%)}table tbody tr td{padding:.5rem}img{border-radius:8px;max-width:100%;height:auto;max-height:510px}code{background-color:var(--blue-50);border-radius:2px;padding:2px 5px}pre{border-radius:8px;padding:1.5em;white-space:pre!important}pre>code{all:unset;font-size:16px}blockquote{border-left:4px solid var(--accent);margin:0;padding:0 0 0 20px;font-size:1.333em}hr{border:none;border-top:1px solid rgb(var(--gray-light))}svg[role="graphics-document document"]{justify-self:center;margin-bottom:1em}@media (max-width:720px){body{font-size:18px}main{padding:1em 0}p{text-align:justify}.line.diff.remove,.line.diff.add{padding:4px 8px 8px!important}.prose{max-width:100%!important;margin:0!important}}.line.diff.remove{background-color:#f7cccc1a;padding:7px 8px 8px;line-height:0}.line.diff.add{background-color:#00ff001a;padding:7px 8px 8px;line-height:0}.sr-only{clip:rect(1px 1px 1px 1px);clip:rect(1px,1px,1px,1px);clip-path:inset(50%);white-space:nowrap;border:0;width:1px;height:1px;margin:0;padding:0;overflow:hidden;position:absolute!important}@property --tw-rotate-x{syntax:"*";inherits:false}@property --tw-rotate-y{syntax:"*";inherits:false}@property --tw-rotate-z{syntax:"*";inherits:false}@property --tw-skew-x{syntax:"*";inherits:false}@property --tw-skew-y{syntax:"*";inherits:false}@property --tw-space-y-reverse{syntax:"*";inherits:false;initial-value:0}@property --tw-border-style{syntax:"*";inherits:false;initial-value:solid}@property --tw-leading{syntax:"*";inherits:false}@property --tw-font-weight{syntax:"*";inherits:false}@property --tw-shadow{syntax:"*";inherits:false;initial-value:0 0 #0000}@property --tw-shadow-color{syntax:"*";inherits:false}@property --tw-shadow-alpha{syntax:"<percentage>";inherits:false;initial-value:100%}@property --tw-inset-shadow{syntax:"*";inherits:false;initial-value:0 0 #0000}@property --tw-inset-shadow-color{syntax:"*";inherits:false}@property --tw-inset-shadow-alpha{syntax:"<percentage>";inherits:false;initial-value:100%}@property --tw-ring-color{syntax:"*";inherits:false}@property --tw-ring-shadow{syntax:"*";inherits:false;initial-value:0 0 #0000}@property --tw-inset-ring-color{syntax:"*";inherits:false}@property --tw-inset-ring-shadow{syntax:"*";inherits:false;initial-value:0 0 #0000}@property --tw-ring-inset{syntax:"*";inherits:false}@property --tw-ring-offset-width{syntax:"<length>";inherits:false;initial-value:0}@property --tw-ring-offset-color{syntax:"*";inherits:false;initial-value:#fff}@property --tw-ring-offset-shadow{syntax:"*";inherits:false;initial-value:0 0 #0000}@property --tw-outline-style{syntax:"*";inherits:false;initial-value:solid}a[data-astro-cid-eimmu3lg]{display:inline-block;text-decoration:none}a[data-astro-cid-eimmu3lg].active{font-weight:bolder;text-decoration:underline}header[data-astro-cid-3ef6ksr2]{margin:0;padding:0 1em;background:#fff;box-shadow:0 2px 8px rgba(var(--black),5%)}h2[data-astro-cid-3ef6ksr2]{margin:0;font-size:1em}h2[data-astro-cid-3ef6ksr2] a[data-astro-cid-3ef6ksr2],h2[data-astro-cid-3ef6ksr2] a[data-astro-cid-3ef6ksr2].active{text-decoration:none}nav[data-astro-cid-3ef6ksr2]{display:flex;align-items:center;justify-content:space-between}nav[data-astro-cid-3ef6ksr2] a[data-astro-cid-3ef6ksr2]{padding:1em .5em;color:var(--black);border-bottom:4px solid transparent;text-decoration:none}nav[data-astro-cid-3ef6ksr2] a[data-astro-cid-3ef6ksr2].active{text-decoration:none;border-bottom-color:var(--accent)}.social-links[data-astro-cid-3ef6ksr2],.social-links[data-astro-cid-3ef6ksr2] a[data-astro-cid-3ef6ksr2]{display:flex;justify-content:center;align-items:center}@media (max-width: 720px){.social-links[data-astro-cid-3ef6ksr2]{display:none}}.navbar-logo[data-astro-cid-3ef6ksr2]{display:flex;justify-content:center;align-items:center}footer[data-astro-cid-sz7xmlte]{padding:2em 1em 6em;background:linear-gradient(var(--gray-gradient)) no-repeat;color:rgb(var(--gray));text-align:center}.social-links[data-astro-cid-sz7xmlte]{display:flex;justify-content:center;gap:1em;margin-top:1em}.social-links[data-astro-cid-sz7xmlte] a[data-astro-cid-sz7xmlte]{text-decoration:none;color:rgb(var(--gray))}.social-links[data-astro-cid-sz7xmlte] a[data-astro-cid-sz7xmlte]:hover{color:rgb(var(--gray-dark))}main[data-astro-cid-bvzihdzo]{width:calc(100% - 2em);max-width:100%;margin:auto}.hero-image[data-astro-cid-bvzihdzo]{width:100%}.hero-image[data-astro-cid-bvzihdzo] img[data-astro-cid-bvzihdzo]{display:block;margin:0 auto;border-radius:12px;box-shadow:var(--box-shadow)}.prose[data-astro-cid-bvzihdzo]{width:56rem;max-width:calc(100% - 2em);margin:auto;padding:1em;color:rgb(var(--gray-dark))}.title[data-astro-cid-bvzihdzo]{margin-bottom:1em;padding:1em 0;text-align:center;line-height:1}.title[data-astro-cid-bvzihdzo] h1[data-astro-cid-bvzihdzo]{margin:0 0 .5em}.date[data-astro-cid-bvzihdzo]{margin-bottom:.5em;color:rgb(var(--gray))}.last-updated-on[data-astro-cid-bvzihdzo]{font-style:italic}main[data-astro-cid-5tznm7mj]{width:960px}ul[data-astro-cid-5tznm7mj]{display:flex;flex-wrap:wrap;gap:2rem;list-style-type:none;margin:0;padding:0}ul[data-astro-cid-5tznm7mj] li[data-astro-cid-5tznm7mj]{width:calc(50% - 1rem)}ul[data-astro-cid-5tznm7mj] li[data-astro-cid-5tznm7mj] [data-astro-cid-5tznm7mj]{text-decoration:none;transition:.2s ease}ul[data-astro-cid-5tznm7mj] li[data-astro-cid-5tznm7mj]:first-child{width:100%;margin-bottom:1rem;text-align:center}ul[data-astro-cid-5tznm7mj] li[data-astro-cid-5tznm7mj]:first-child img[data-astro-cid-5tznm7mj]{width:100%}ul[data-astro-cid-5tznm7mj] li[data-astro-cid-5tznm7mj]:first-child .title[data-astro-cid-5tznm7mj]{font-size:2.369rem}ul[data-astro-cid-5tznm7mj] li[data-astro-cid-5tznm7mj] img[data-astro-cid-5tznm7mj]{margin-bottom:1.5rem;border-radius:12px}ul[data-astro-cid-5tznm7mj] li[data-astro-cid-5tznm7mj] a[data-astro-cid-5tznm7mj]{display:block}.title[data-astro-cid-5tznm7mj]{margin:0;color:rgb(var(--black));line-height:1.3}.date[data-astro-cid-5tznm7mj]{margin:0;color:rgb(var(--gray))}ul[data-astro-cid-5tznm7mj] li[data-astro-cid-5tznm7mj] a[data-astro-cid-5tznm7mj]:hover h4[data-astro-cid-5tznm7mj],ul[data-astro-cid-5tznm7mj] li[data-astro-cid-5tznm7mj] a[data-astro-cid-5tznm7mj]:hover .date[data-astro-cid-5tznm7mj]{color:rgb(var(--accent))}ul[data-astro-cid-5tznm7mj] a[data-astro-cid-5tznm7mj]:hover img[data-astro-cid-5tznm7mj]{box-shadow:var(--box-shadow)}ul[data-astro-cid-5tznm7mj] li[data-astro-cid-5tznm7mj]:not(:first-child) img[data-astro-cid-5tznm7mj]{height:240px}@media (max-width: 720px){ul[data-astro-cid-5tznm7mj]{gap:.5em}ul[data-astro-cid-5tznm7mj] li[data-astro-cid-5tznm7mj]{width:100%;text-align:center}ul[data-astro-cid-5tznm7mj] li[data-astro-cid-5tznm7mj]:first-child{margin-bottom:0}ul[data-astro-cid-5tznm7mj] li[data-astro-cid-5tznm7mj]:first-child .title[data-astro-cid-5tznm7mj]{font-size:1.563em}}main[data-astro-cid-2iqln3bm]{width:calc(100% - 2em);max-width:100%;margin:0}ul[data-astro-cid-2iqln3bm]{list-style-type:disc;padding-left:1.5em}ul[data-astro-cid-2iqln3bm] ul[data-astro-cid-2iqln3bm],ul[data-astro-cid-2iqln3bm] ul[data-astro-cid-2iqln3bm] ul[data-astro-cid-2iqln3bm]{list-style-type:circle}.hero-image[data-astro-cid-2iqln3bm]{width:100%}.hero-image[data-astro-cid-2iqln3bm] img[data-astro-cid-2iqln3bm]{display:block;margin:0 auto;border-radius:12px;box-shadow:var(--box-shadow)}.prose[data-astro-cid-2iqln3bm]{width:56rem;max-width:calc(100% - 2em);margin:auto;padding:1em;color:rgb(var(--gray-dark))}.title[data-astro-cid-2iqln3bm]{margin-bottom:1em;padding:1em 0;text-align:center;line-height:1}.title[data-astro-cid-2iqln3bm] h1[data-astro-cid-2iqln3bm]{margin:0 0 .5em}.date[data-astro-cid-2iqln3bm]{margin-bottom:.5em;color:rgb(var(--gray))}.last-updated-on[data-astro-cid-2iqln3bm]{font-style:italic}a[data-astro-cid-jwgesljd],a[data-astro-cid-jwgesljd]:hover{color:#4a5565}
</style></head> <body data-astro-cid-bvzihdzo> <header data-astro-cid-3ef6ksr2> <nav data-astro-cid-3ef6ksr2> <h2 class="navbar-logo" data-astro-cid-3ef6ksr2> <img width="40" height="40" src="/logo.png" alt="" data-astro-cid-3ef6ksr2> <a href="/" class="font-bold text-2xl" data-astro-cid-3ef6ksr2>BigJoe</a> </h2> <div class="internal-links" data-astro-cid-3ef6ksr2> <a href="/" class="active" data-astro-cid-3ef6ksr2="true" data-astro-cid-eimmu3lg> Blog </a>  <a href="/book" data-astro-cid-3ef6ksr2="true" data-astro-cid-eimmu3lg> Book </a>  </div> <div class="social-links" data-astro-cid-3ef6ksr2> <a href="https://www.facebook.com/profile.php?id=61572916280350" target="_blank" data-astro-cid-3ef6ksr2> <svg xmlns="http://www.w3.org/2000/svg" width="40" height="40" viewBox="0 0 24 24" data-astro-cid-3ef6ksr2> <path fill="currentColor" d="M13.458 21.696c4.693-.704 8.292-4.753 8.292-9.642c0-5.385-4.365-9.75-9.75-9.75s-9.75 4.365-9.75 9.75c0 4.89 3.599 8.938 8.292 9.642v-6.798h-2.05a.486.486 0 0 1-.486-.486v-1.843c0-.269.218-.486.486-.486h2.05l-.072-1.943c0-.942.175-2.471 1.342-3.307c.816-.583 1.423-.693 2.397-.693c.845 0 1.426.084 1.81.14l.188.025a.193.193 0 0 1 .168.192v2.04c0 .113-.095.2-.205.194h-.038c-.114.004-.71.029-1.216.029c-.89 0-1.458.406-1.458 1.755v1.568h2.192c.3 0 .529.27.48.566l-.28 1.843a.486.486 0 0 1-.479.406h-1.913z" data-astro-cid-3ef6ksr2></path> </svg> </a> <a href="https://x.com/joewalker_xyz" target="_blank" data-astro-cid-3ef6ksr2> <svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24" data-astro-cid-3ef6ksr2> <g fill="none" fill-rule="evenodd" data-astro-cid-3ef6ksr2> <path d="m12.594 23.258l-.012.002l-.071.035l-.02.004l-.014-.004l-.071-.036q-.016-.004-.024.006l-.004.01l-.017.428l.005.02l.01.013l.104.074l.015.004l.012-.004l.104-.074l.012-.016l.004-.017l-.017-.427q-.004-.016-.016-.018m.264-.113l-.014.002l-.184.093l-.01.01l-.003.011l.018.43l.005.012l.008.008l.201.092q.019.005.029-.008l.004-.014l-.034-.614q-.005-.019-.02-.022m-.715.002a.02.02 0 0 0-.027.006l-.006.014l-.034.614q.001.018.017.024l.015-.002l.201-.093l.01-.008l.003-.011l.018-.43l-.003-.012l-.01-.01z" data-astro-cid-3ef6ksr2></path> <path fill="currentColor" d="M19.753 4.659a1 1 0 0 0-1.506-1.317l-5.11 5.84L8.8 3.4A1 1 0 0 0 8 3H4a1 1 0 0 0-.8 1.6l6.437 8.582l-5.39 6.16a1 1 0 0 0 1.506 1.317l5.11-5.841L15.2 20.6a1 1 0 0 0 .8.4h4a1 1 0 0 0 .8-1.6l-6.437-8.582l5.39-6.16ZM16.5 19L6 5h1.5L18 19z" data-astro-cid-3ef6ksr2></path> </g> </svg> </a> <a href="https://github.com/joeteerawit" target="_blank" data-astro-cid-3ef6ksr2> <svg viewBox="0 0 16 16" aria-hidden="true" width="32" height="32" data-astro-cid-3ef6ksr2><path fill="currentColor" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8z" data-astro-cid-3ef6ksr2></path></svg> </a> </div> </nav> </header>  <main data-astro-cid-bvzihdzo> <article data-astro-cid-bvzihdzo> <div class="hero-image" data-astro-cid-bvzihdzo> <img width="1020" height="510" src="/2025/05/65ae96c9-5f02-4c83-a35d-d400efcda942.png" alt="" data-astro-cid-bvzihdzo> </div> <div class="prose" data-astro-cid-bvzihdzo> <div class="title" data-astro-cid-bvzihdzo> <div class="date" data-astro-cid-bvzihdzo> <time datetime="2025-05-10T00:00:00.000Z"> May 10, 2025 </time>  </div> <h1 data-astro-cid-bvzihdzo>Facebook รองรับ Request Billion/Sec โดยใช้ Memcached ได้อย่างไร?</h1> <hr data-astro-cid-bvzihdzo> </div>  <blockquote>
<p>Note: Paper นี้ public เมื่อตอนปี 2013 นะ</p>
</blockquote>
<p>มีความจริงอยู่สองข้อที่เลี่ยงไม่ได้เลย ถ้าจะรัน social network ที่ใหญ่ระดับ Facebook:</p>
<p><strong>หนึ่งคือ ระบบล่มไม่ได้</strong><br>
<strong>สองคือ ระบบช้าไม่ได้</strong></p>
<p>สองข้อนี้แหละเป็นตัวตัดสินเลยว่า คนจะอยู่ต่อใน social network ของเราหรือไม่</p>
<p>แค่มีคนออกไปไม่กี่คน ก็อาจส่งผลกับทั้งระบบผู้ใช้ เพราะทุกคนมันเชื่อมกันหมด คนส่วนใหญ่ก็เล่นเพราะเพื่อนหรือญาติก็เล่น ถ้าใครคนหนึ่งหลุดไปเพราะเจอปัญหา ก็อาจลากเพื่อนคนอื่น ๆ หายไปด้วยเหมือนโดมิโน</p>
<p>Facebook เจอปัญหานี้ตั้งแต่ช่วงแรก ๆ เพราะคนเล่นเยอะมาก ระดับที่มีคนนับล้านเข้าใช้งานจากทั่วโลกในเวลาเดียวกัน</p>
<p>พอเป็นแบบนี้ การออกแบบ software เลยต้องมีเงื่อนไขที่สำคัญหลายอย่าง:</p>
<ul>
<li>Facebook ต้องรองรับ <strong>real-time communication</strong></li>
<li>ต้องมีระบบที่ <strong>aggregate content แบบ on-the-fly</strong> ได้เลย</li>
<li>ต้อง <strong>scale</strong> เพื่อรับ <strong>user requests หลัก billions</strong></li>
<li>และต้อง <strong>เก็บข้อมูลเป็น trillions ชิ้น</strong> ข้ามหลายประเทศ</li>
</ul>
<p>เพื่อให้ไปถึงเป้าหมายพวกนี้ Facebook เลยเอา <strong>Memcached</strong> ที่เป็น open-source มาใช้งาน แล้วปรับแต่งให้มันกลายเป็น <strong>distributed key-value store</strong></p>
<p>version ที่ปรับปรุงแล้วนี้เรียกว่า <strong>Memcache</strong> (ไม่มี - <code>d</code>)</p>
<p>โพสต์นี้เราจะพาไปดูว่า Facebook จัดการยังไงกับปัญหาในการ scale memcached ให้รองรับ <strong>requests ระดับ billions ครั้งต่อวินาที</strong> ได้ยังไงบ้าง</p>
<h2 id="memcached">Memcached</h2>
<p>คือ in-memory key-value store ที่รองรับคำสั่งพื้นฐานอย่าง <code>set</code>, <code>get</code>, และ <code>delete</code></p>
<p>version open-source ที่ปล่อยออกมา จะเป็น in-memory hash table สำหรับเครื่องเดียว ซึ่งทีม engineers ของ Facebook ก็หยิบ version นี้มาใช้เป็นแกนหลัก แล้วขยายมันให้กลายเป็นระบบ key-value store แบบกระจาย (distributed) ที่เรียกว่า <strong>Memcache</strong></p>
<p>พูดง่าย ๆ ก็คือ “<strong>Memcached</strong>” คือ source code หรือ binary ที่รันได้จริง ส่วน “<strong>Memcache</strong>” คือระบบ distributed ที่อยู่เบื้องหลังมัน</p>
<p>จากมุม technical, Facebook ใช้ <strong>Memcache</strong> อยู่ 2 แบบหลัก ๆ:</p>
<h2 id="query-cache">Query Cache</h2>
<p>หน้าที่ของ query cache คือช่วยลดภาระ load จาก database ตัวหลัก (source-of-truth)</p>
<p>ในโหมดนี้ Facebook ใช้ Memcache แบบที่เรียกว่า <strong>demand-filled look-aside cache</strong> หรือบางคนอาจจะรู้จักในชื่อ <strong>cache-aside pattern</strong></p>
<p>ลองดูจาก diagram จะเห็นว่า look-aside cache มันทำงานยังไง ทั้งฝั่งอ่าน (read) และเขียน (write)</p>
<p><img src="/2025/05/e23db5b3-7227-440a-a452-7ceb28479475.webp" alt=""></p>
<p>ฝั่ง <strong>read</strong> จะใช้ cache แบบโหลดตามความต้องการ คือจะโหลดข้อมูลเข้า cache เฉพาะเวลาที่ client ขอเท่านั้น</p>
<p>ก่อนจะเสิร์ฟข้อมูลให้ client, ระบบจะเช็ค cache ก่อน ถ้ามีข้อมูลอยู่ก็จบเลย แต่ถ้าไม่มี (cache miss) client ก็จะไปดึงข้อมูลจาก database แล้วค่อยเอามาเก็บใน cache ทีหลัง</p>
<p>ส่วนฝั่ง <strong>write</strong> จะมีเทคนิคที่น่าสนใจกว่านั้นหน่อย</p>
<p>หลังจากที่มีการ update key บางตัวใน database, ระบบจะ <strong>ไม่</strong> ไป update ค่าใน cache โดยตรง แต่จะ <strong>ลบ</strong> ข้อมูล key นั้นออกจาก cache ไปเลย วิธีนี้เรียกว่า <strong>cache invalidation</strong></p>
<p>การ invalidate cache entry จะช่วยให้ระบบมั่นใจได้ว่า ครั้งต่อไปที่ client ขอข้อมูลของ key นั้น<br>
มันจะเจอ <strong>cache miss</strong> และต้องไปดึงข้อมูลล่าสุดโดยตรงจาก <strong>database</strong></p>
<p>วิธีนี้ช่วยให้ระบบคง <strong>data consistency ระหว่าง cache กับ database</strong> เอาไว้ได้</p>
<h2 id="generic-cache">Generic Cache</h2>
<p>Facebook ใช้ <strong>Memcache</strong> เป็น key-value store แบบทั่วไปด้วยนะ ซึ่งทำให้ทีมต่าง ๆ ในบริษัทสามารถเอา Memcache มาเก็บพวกผลลัพธ์ที่ประมวลผลไว้ล่วงหน้า (pre-computed results) จาก algorithm พวก machine learning ที่กิน resource หนัก ๆ ได้เลย</p>
<p>พอเก็บไว้ใน Memcache แล้ว app อื่น ๆ ก็สามารถเข้าถึงผลลัพธ์พวกนี้ได้แบบเร็วและง่ายมาก</p>
<p>แนวทางนี้ช่วยให้ performance ดีขึ้น แถมยังใช้ resource ได้คุ้มค่ากว่าเดิมด้วย</p>
<h2 id="high-level-architecture-of-facebook">High-Level Architecture of Facebook</h2>
<p>Facebook ออกแบบสถาปัตยกรรมระบบมาเพื่อรับมือกับ scale ที่ใหญ่มหาศาล และการใช้งานระดับทั่วโลก
ในช่วงที่เริ่มนำ <strong>Memcached</strong> มาใช้ สถาปัตยกรรมระดับสูงของ Facebook แบ่งออกเป็น 3 ส่วนหลัก ๆ:</p>
<h2 id="1---regions">1 - Regions</h2>
<p>Facebook วาง server ตามจุดยุทธศาสตร์ทั่วโลก ซึ่งเรียกว่าพวก <strong>region</strong> โดยจะแบ่งออกเป็น 2 ประเภท:</p>
<p><strong>Primary Region</strong>: ทำหน้าที่หลักในการรับ traffic จากผู้ใช้ และจัดการข้อมูลต่าง ๆ
<strong>Secondary Region</strong>: กระจายตัวอยู่ทั่วโลก เพื่อช่วยเรื่อง redundancy, load balancing และเพิ่ม performance ให้ผู้ใช้ในแต่ละพื้นที่</p>
<p>ไม่ว่าจะเป็น primary หรือ secondary, region ทุกอันจะมี <strong>frontend cluster หลายชุด</strong> และ <strong>storage cluster หนึ่งชุด</strong></p>
<h2 id="2---frontend-clusters">2 - Frontend Clusters</h2>
<p>ในแต่ละ region, Facebook จะมี <strong>frontend cluster</strong> ไว้จัดการกับ request จากผู้ใช้ และให้บริการเนื้อหา</p>
<p>1 frontend cluster จะประกอบด้วย 2 ส่วนหลัก:</p>
<ul>
<li><strong>Web Servers</strong>: จัดการ request จากผู้ใช้, render หน้าเว็บ, และส่งเนื้อหากลับไปให้ผู้ใช้</li>
<li><strong>Memcache Servers</strong>: ทำหน้าที่เป็น <strong>distributed caching layer</strong> เก็บข้อมูลที่ถูกเรียกใช้บ่อย ๆ ไว้ใน memory เพื่อให้ดึงมาใช้ได้เร็ว</li>
</ul>
<p>Frontend cluster เหล่านี้ถูกออกแบบให้สามารถ <strong>scale แนวนอน (horizontal scaling)</strong> ได้ตามความต้องการ<br>
ถ้า traffic เพิ่ม ก็สามารถเพิ่ม web server หรือ Memcache server เข้าไปใน cluster ได้ทันที เพื่อรองรับโหลดที่มากขึ้น</p>
<h3 id="3---storage-cluster">3 - Storage Cluster</h3>
<p>ที่ใจกลางของแต่ละ region จะมี <strong>storage cluster</strong> ซึ่งเป็นที่อยู่ของ <strong>source-of-truth database</strong> หรือก็คือฐานข้อมูลที่เก็บข้อมูลตัวจริงทั้งหมดของระบบ Facebook
storage cluster ตัวนี้ทำหน้าที่ดูแลทั้ง <strong>data consistency</strong>, <strong>durability</strong>, และ <strong>reliability</strong></p>
<p>Facebook ใช้การ <strong>replicate ข้อมูลข้ามหลาย region</strong> และออกแบบระบบแบบ <strong>primary-secondary architecture</strong><br>
ซึ่งช่วยให้ระบบมี <strong>high availability</strong> และสามารถ <strong>ทนต่อความเสียหาย (fault tolerance)</strong> ได้ดี</p>
<p>ในไดอะแกรมข้างล่าง (ต้นฉบับ) จะเห็นภาพรวมของ architecture นี้ได้ชัดเจน
<img src="/2025/05/d37d0355-7214-4deb-a48e-0dcc719cbf1f.webp" alt=""></p>
<p>หนึ่งในแนวคิดหลักที่ Facebook เลือกใช้คือการ <strong>ยอมแสดงข้อมูลที่อาจจะ stale (เก่าเล็กน้อย)</strong> ดีกว่าให้ backend รับโหลดหนักจนล่ม</p>
<p>แทนที่จะพยายามให้ข้อมูลทุกอย่าง consistent 100% ตลอดเวลา<br>
Facebook ยอมให้ผู้ใช้เห็นข้อมูลที่อาจจะยังไม่ update ใน feed บ้าง<br>
เพื่อแลกกับความสามารถในการรองรับ traffic จำนวนมหาศาลโดยที่ระบบยังทำงานต่อได้ ไม่พัง</p>
<p>เพื่อให้ architecture แบบนี้สามารถรองรับระดับ <strong>request เป็น billions ครั้งต่อวัน</strong> ได้จริง Facebook ต้องแก้ปัญหาใหญ่หลายด้าน เช่น:</p>
<ul>
<li>การจัดการ <strong>latency และ failure ภายใน cluster</strong></li>
<li>การจัดการ <strong>data replication ภายใน region</strong></li>
<li>การจัดการ <strong>data consistency ระหว่าง region ต่าง ๆ</strong></li>
</ul>
<p>ใน section ต่อไป เราจะมาดูกันว่า Facebook จัดการกับความท้าทายเหล่านี้ยังไงบ้าง</p>
<h2 id="แนวคิดที่-facebook-ใช้">แนวคิดที่ Facebook ใช้</h2>
<p>Facebook มีปรัชญาหนึ่งที่สำคัญมาก คือ <strong>ยอมให้ข้อมูล stale นิดหน่อย ดีกว่าทำให้ backend พังเพราะโหลดหนัก</strong></p>
<p>แทนที่จะพยายามให้ข้อมูลทุกอย่าง fresh และ consistent ตลอดเวลา, Facebook ยอมให้ feed ของ user อาจเห็นข้อมูลที่ไม่ update บ้าง เพื่อแลกกับการที่ระบบไม่ล่มเวลาคนเข้าเยอะ ๆ</p>
<p>การจะทำ architecture แบบนี้ให้รอดในระดับ <strong>request เป็น billions ครั้งต่อวัน</strong> ต้องแก้โจทย์ใหญ่หลายอย่าง เช่น:</p>
<ul>
<li>จัดการ latency กับ failure ใน cluster</li>
<li>จัดการการ replicate ข้อมูลใน region เดียวกัน</li>
<li>จัดการ consistency ของข้อมูลระหว่าง region ต่าง ๆ</li>
</ul>
<p>เดี๋ยวใน section ต่อ ๆ ไป เราจะไปดูว่า Facebook จัดการกับแต่ละปัญหานี้ยังไง</p>
<h2 id="within-cluster-challenges">Within Cluster Challenges</h2>
<p>ในระดับ cluster ภายใน Facebook ตั้งเป้าไว้ 3 อย่างหลัก ๆ:</p>
<ul>
<li>ลด latency</li>
<li>ลด load บน database</li>
<li>จัดการกับปัญหาเวลามี failure</li>
</ul>
<h3 id="1---reducing-latency">1 - Reducing Latency</h3>
<p>อย่างที่พูดไปก่อนหน้านี้ว่าในแต่ละ frontend cluster จะมี Memcached server อยู่เป็นร้อย ๆ ตัว แล้วข้อมูลก็ถูกกระจายเก็บตาม server พวกนี้ด้วยเทคนิคที่เรียกว่า <strong>Consistent Hashing</strong></p>
<p>พูดให้เข้าใจง่าย ๆ <strong>Consistent Hashing</strong> คือเทคนิคที่ช่วยกระจาย key ไปตาม node ต่าง ๆ โดยที่เวลา node ใด node หนึ่งพังหรือมี node ใหม่เพิ่มเข้ามา จะไม่ต้องย้ายข้อมูลทั้งหมดใหม่ — ย้ายแค่บางส่วนพอ</p>
<p>ในไดอะแกรม (จากต้นฉบับ) จะเห็นว่ามีการ map key ลงไปบนวงกลม hash space แล้วแต่ละ node จะถูก assign ตำแหน่งบนวงนั้น ซึ่ง key ใด ๆ จะถูก assign ไปที่ node ที่อยู่ถัดไปในทิศตามเข็มนาฬิกา
<img src="/2025/05/07f092ce-0d07-4d9a-9aaf-e3b92b00a736.webp" alt=""></p>
<p>ใน scale ของ Facebook การ request หน้าเว็บแค่ครั้งเดียวอาจทำให้ต้องดึงข้อมูลหลายร้อยรายการจาก Memcached servers ตัวอย่างเช่น หน้า feed ที่มี post กับ comment เยอะ ๆ</p>
<p>แค่ request เดียว ก็อาจทำให้ web server ต้องคุยกับ Memcached หลายตัวภายในไม่กี่มิลลิวินาที เพื่อโหลดข้อมูลทั้งหมดที่ต้องใช้</p>
<p>การดึงข้อมูลเยอะ ๆ แบบนี้เกิดได้ทั้งตอนที่มี <strong>cache hit</strong> และตอนที่เจอ <strong>cache miss</strong> ด้วย<br>
ซึ่งหมายความว่า Memcached server ตัวเดียวก็อาจกลายเป็น <strong>bottleneck</strong> ให้กับหลาย ๆ web server ได้เลย ส่งผลให้ latency สูงขึ้นและทำให้ user ได้ประสบการณ์ที่ช้าลง</p>
<p>เพื่อป้องกันปัญหาแบบนี้ Facebook เลยมีเทคนิคหลายอย่างมาช่วยลดโอกาสที่มันจะเกิด ซึ่งอธิบายไว้ในไดอะแกรม (ในต้นฉบับ)
<img src="/2025/05/64de4d22-5932-4fa9-b82c-cdf7c740183d.webp" alt=""></p>
<h2 id="parallel-requests-and-batching">Parallel Requests and Batching</h2>
<p>จะเข้าใจแนวคิดของ parallel requests กับ batching ง่ายขึ้น ถ้าลองนึกภาพตามแบบนี้:</p>
<p>สมมุติว่าทุกครั้งที่อยากได้ของ 1 ชิ้น ต้องเดินไป supermarket ใหม่ทุกครั้ง — แค่คิดก็น่าเหนื่อยละ เสียเวลาแถมไม่คุ้มเลย<br>
วิธีที่ดีกว่าคือวางแผนล่วงหน้า แล้วซื้อของหลาย ๆ อย่างทีเดียวรอบเดียวจบ</p>
<p>Facebook ก็ใช้หลักการ optimization แบบเดียวกันกับการดึงข้อมูลใน frontend cluster</p>
<p>เพื่อให้ดึงข้อมูลได้มีประสิทธิภาพมากที่สุด Facebook จะสร้าง <strong>Directed Acyclic Graph (DAG)</strong> ที่แสดง dependency ระหว่าง data แต่ละตัว</p>
<p>DAG จะช่วยบอกว่า data ไหนสามารถดึงได้พร้อมกัน และ data ไหนต้องรอก่อนเพราะมี dependency</p>
<p>เมื่อ web server วิเคราะห์ DAG แล้ว มันก็จะรู้ว่า fetch request ไหนสามารถทำ <strong>แบบ parallel</strong> ได้บ้าง และ grouping เป็น batch เดียวกันได้ยังไงบ้าง เพื่อให้ดึงข้อมูลได้เร็วที่สุดและมีประสิทธิภาพที่สุด</p>
<h3 id="using-udp">Using UDP</h3>
<p>Facebook มีเทคนิคเด็ดในการ optimize การสื่อสารระหว่าง web server กับ Memcache server ด้วยการใช้ <strong>UDP แทน TCP</strong> สำหรับ fetch requests</p>
<p>อย่างที่รู้กัน UDP เป็น protocol ที่ไม่ต้องเปิด connection และเร็วกว่ามากเมื่อเทียบกับ TCP<br>
เพราะแบบนี้ client สามารถส่ง fetch request ไปที่ Memcache server ได้เร็วขึ้น โดยใช้ network resource น้อยลง ทำให้ latency ต่ำลง</p>
<p>แต่ UDP ก็มีข้อเสียคือ มัน <strong>ไม่รับประกันว่า packet จะถูกส่งถึงแน่นอน</strong><br>
ถ้า packet หายระหว่างทาง UDP ก็จะไม่พยายามส่งซ้ำให้</p>
<p>Facebook เลยตั้งหลักไว้ว่า ถ้าไม่เห็น response ภายในเวลาที่กำหนด ก็ถือว่า <strong>cache miss ไปเลย</strong> แล้วให้ client ไปดึงข้อมูลจากแหล่งหลักแทน</p>
<p>ส่วน operation อย่าง <strong>update กับ delete</strong> ยังใช้ <strong>TCP อยู่</strong> เพราะมันต้องการความแน่นอนว่า packet ส่งถึงแน่และเรียงลำดับถูก ซึ่งสำคัญมากเวลาจัดการกับข้อมูลที่ต้องเปลี่ยนแปลง</p>
<p>ทั้งหมดนี้จะถูกจัดการผ่าน <strong>proxy พิเศษที่ชื่อว่า <code>mcrouter</code></strong> ซึ่งรันอยู่บนเครื่องเดียวกับ webserver<br>
คิดง่าย ๆ ว่า mcrouter ทำหน้าที่เป็นคนกลาง ที่ช่วยจัดการหลายเรื่อง เช่น data serialization, compression, routing, batching และ error handling<br>
เดี๋ยวเราจะมาพูดถึง mcrouter แบบเต็ม ๆ ใน section ถัดไป</p>
<h2 id="2---reducing-load">2 - Reducing Load</h2>
<p>เป้าหมายหลักของ Memcache คือ <strong>ลดภาระของ database</strong> โดยลดจำนวนครั้งที่ต้องไปดึงข้อมูลจาก database จริง ๆ</p>
<p>การใช้ Memcache แบบ <strong>look-aside cache</strong> ก็ช่วยเรื่องนี้ได้เยอะมากอยู่แล้ว<br>
แต่ในระดับ scale ของ Facebook มันจะมีปัญหาคลาสสิกของ caching ที่โผล่มาแน่นอน 2 อย่าง:</p>
<p><strong>Stale Set</strong> – คือ cache มีข้อมูลที่เก่าแล้ว แต่ไม่มีวิธี invalidate ง่าย ๆ<br>
<strong>Thundering Herd</strong> – เกิดเวลาเจอ cache miss แล้วมี request จำนวนมหาศาลวิ่งไปหา database พร้อมกัน</p>
<p>ในไดอะแกรม (จากต้นฉบับ) จะเห็นภาพชัดขึ้นว่า 2 ปัญหานี้หน้าตาเป็นยังไง</p>
<p><img src="/2025/05/5719f9ad-dca7-4842-8588-6dcd10b56407.webp" alt=""></p>
<p>เพื่อรับมือกับปัญหานี้ Facebook ใช้เทคนิคที่เรียกว่า <strong>leasing</strong></p>
<p><strong>Leasing</strong> ช่วยแก้ได้ทั้ง stale set และ thundering herd ทำให้ Facebook ลด load บน database ตอนพีค ๆ จาก <strong>17,000 queries/sec เหลือแค่ 1,300 queries/sec</strong></p>
<h2 id="stale-sets">Stale Sets</h2>
<p>ลองนึกภาพว่า client ขอข้อมูลจาก memcache ด้วย key หนึ่ง แล้วเจอ cache miss</p>
<p>ในกรณีแบบนี้ client จะต้องเป็นคนไปดึงข้อมูลจาก database เอง แล้ว update เข้า memcache ด้วย เพื่อให้ request ต่อไปไม่เจอ cache miss อีก</p>
<p>ฟังดูโอเคใช่มั้ย แต่ในระบบที่ concurrent สูง ๆ ข้อมูลที่ client กำลังจะเซตเข้า cache อาจ <strong>เก่าไปแล้ว</strong> ตอนที่มันเขียนเข้าไปจริง ๆ</p>
<p>Facebook เลยใช้เทคนิคที่เรียกว่า <strong>leasing</strong> เพื่อกันปัญหานี้</p>
<p>เวลามี cache miss, Memcache จะออก <strong>lease</strong> (เป็น token แบบ 64-bit ผูกกับ key นั้น) ให้ client หนึ่งราย เพื่อให้มีสิทธิ์เซตข้อมูลกลับเข้าไปใน cache</p>
<p>พอ client จะเซตข้อมูล มันต้องแนบ lease token ไปด้วย Memcache ก็จะเช็คว่า token ยังใช้ได้มั้ย ถ้า key นั้นถูก invalidate ไปก่อนแล้ว Memcache จะปฏิเสธการเซต และถือว่า lease token นั้นใช้ไม่ได้อีก</p>
<p>ในไดอะแกรม (ของต้นฉบับ) จะเห็นภาพว่า leasing ทำงานยังไง
<img src="/2025/05/5634649e-3bb0-4028-ab8c-d9e0eed405bf.webp" alt=""></p>
<h2 id="thundering-herds">Thundering Herds</h2>
<p>Facebook ยังใช้การดัดแปลงเล็ก ๆ ของ <strong>leasing</strong> เพื่อแก้ปัญหา <strong>thundering herd</strong> ด้วย</p>
<p>ใน version  นี้ Memcache จะคอยควบคุมว่า key แต่ละอันจะออก lease token ได้บ่อยแค่ไหน เช่น อาจจะให้แค่ทุก 5 วินาทีต่อ key</p>
<p>ถ้ามี request เข้ามาหลายตัวภายใน 5 วินาทีหลังจาก token ตัวแรกถูกออก Memcache จะตอบกลับไปว่า “รอก่อนนะ” เพื่อให้ client เหล่านั้นไม่รีบยิง request เข้า database ทันที<br>
เพราะมีโอกาสสูงมากว่า client ที่ถือ lease อยู่จะเซตข้อมูลเข้า cache เร็ว ๆ นี้ และ client ที่รอก็จะได้ cache hit ไปเลยตอน retry</p>
<h2 id="3---handling-failures">3 - Handling Failures</h2>
<p>ในระบบขนาดใหญ่มหาศาลแบบ Facebook การเจอ failure เป็นเรื่องปกติมาก</p>
<p>พอมีผู้ใช้เป็นล้าน ๆ คน ถ้ามีปัญหาแค่ดึงข้อมูลจาก Memcache ไม่ได้ ก็อาจลาก backend พังได้ง่าย ๆ เพราะ request จะวิ่งไปที่ server backend เต็ม ๆ จนกลายเป็น <strong>cascading failure</strong></p>
<h3 id="two-levels-of-failure">Two Levels of Failure</h3>
<p>Facebook เจอกับ failure สองระดับหลัก ๆ เวลาใช้ Memcache:</p>
<ul>
<li><strong>Small-Scale Outages</strong>: host จำนวนหนึ่งอาจเจอปัญหา เช่น network ล่มเฉพาะบางเครื่อง ถึงจะเป็นแค่บางจุด แต่ก็ส่งผลกับ performance ได้เหมือนกัน</li>
<li><strong>Widespread Outages</strong>: ถ้าหนักกว่านั้นคือทั้ง cluster ล่ม ส่งผลกับ Memcache hosts จำนวนมาก ซึ่งกระทบกับความเสถียรของระบบแบบชัดเจนเลย</li>
</ul>
<h3 id="handling-widespread-outages">Handling Widespread Outages</h3>
<p>เวลา cluster ล่มแบบรุนแรง Facebook จะใช้วิธี <strong>redirect request ไปที่ cluster อื่นแทน</strong><br>
เพื่อให้ cluster ที่เจอปัญหาได้พักจนกว่าจะกู้ระบบกลับมาได้</p>
<h3 id="automated-remediation-for-small-outages">Automated Remediation for Small Outages</h3>
<p>สำหรับกรณีที่เกิด <strong>outage ขนาดเล็ก</strong>, Facebook ใช้ระบบ <strong>automated remediation</strong><br>
ซึ่งสามารถตรวจเจอปัญหาระดับ host ได้อัตโนมัติ แล้วสร้าง instance ใหม่ขึ้นมาแทนตัวที่มีปัญหา</p>
<p>แต่การ remediate แบบนี้ <strong>ไม่ใช่ทันทีทันใด</strong> — อาจใช้เวลาสักพักกว่าระบบจะกลับมาเป็นปกติ<br>
ระหว่างนั้น backend services อาจต้องรับโหลดมหาศาล เพราะ client พยายามดึงข้อมูลจาก Memcache ที่ล่มอยู่</p>
<p>แนวทางทั่วไปที่มักใช้คือ <strong>rehash key แล้วกระจายไปยัง server ที่เหลือ</strong></p>
<p>แต่ทีม engineers ของ Facebook พบว่า วิธีนี้ยังเสี่ยงต่อ <strong>cascading failure</strong> อยู่ดี<br>
ในระบบของพวกเขา key บางอันอาจกิน traffic ถึง <strong>20% ของ request ทั้งหมดใน server เดียว</strong><br>
ถ้าย้าย key ที่มี traffic หนัก ๆ แบบนี้ไปยัง server อื่นระหว่างเกิดปัญหา มันอาจทำให้ server นั้นพังต่ออีก</p>
<p>เพื่อจัดการกับความเสี่ยงนี้ Facebook เลยใช้แนวทางที่เรียกว่า <strong>Gutter machines</strong><br>
ในแต่ละ cluster จะมีการกันเครื่องไว้ประมาณ <strong>1% ของ Memcache servers</strong><br>
ให้เป็นกลุ่มพิเศษที่เรียกว่า <strong>Gutter pool</strong> ซึ่งพร้อมจะเข้ามารับงานแทน Memcache ที่ล่ม</p>
<ul>
<li>ถ้า client ยิง request แล้วไม่ได้ response กลับมาเลย (แม้แต่ cache miss) — ระบบจะถือว่า server พัง แล้ว redirect ไปที่ <strong>Gutter pool</strong></li>
<li>ถ้า Gutter pool ก็ยัง cache miss, client จะไป query จาก database แล้วเอาข้อมูลใส่ไว้ใน Gutter pool</li>
<li>ข้อมูลใน Gutter จะ <strong>expire เร็วมาก</strong> เพื่อลดความซับซ้อน ไม่ต้องมีการ invalidate ทีหลัง</li>
</ul>
<p>แม้ว่าแนวทางนี้อาจทำให้ user เจอข้อมูลที่ stale บ้าง แต่ข้อดีคือ <strong>backend ไม่โดน overload</strong><br>
และสำหรับ Facebook แล้ว นี่เป็น trade-off ที่ยอมรับได้เพื่อแลกกับ <strong>availability</strong></p>
<p>ในไดอะแกรม (ต้นฉบับ) จะอธิบาย flow การทำงานของ Gutter pool ได้ชัดเจน</p>
<p><img src="/2025/05/90f5c038-a4a6-4036-8fe6-a949d742d714.webp" alt=""></p>
<p>เพื่อป้องกันปัญหาแบบนี้ Facebook เลยใช้แนวทางที่เรียกว่า <strong>Gutter machines</strong></p>
<p>ในแต่ละ cluster จะมีเครื่องสำรองไว้ประมาณ 1% ของ Memcache server ทั้งหมด เพื่อใช้เป็น Gutter pool<br>
Gutter จะทำหน้าที่แทน Memcache ที่ล่มในช่วงนั้น</p>
<p>วิธีทำงานคือ:</p>
<ul>
<li>ถ้า client ส่ง request แล้วไม่เจอแม้แต่ cache miss ก็ถือว่า Memcache server ล่ม แล้วยิง request ไปที่ Gutter pool แทน</li>
<li>ถ้า Gutter pool ก็ไม่มีข้อมูล (cache miss) client จะไป query จาก database แล้วเอาข้อมูลมาใส่ใน Gutter pool เพื่อให้ request รอบหน้าจะได้ข้อมูลจาก cache ได้เลย</li>
<li>ข้อมูลใน Gutter จะ <strong>หมดอายุเร็ว</strong> เพื่อลดความซับซ้อนเรื่องการ invalidate</li>
</ul>
<p>ในไดอะแกรม (ของต้นฉบับ) จะเห็นภาพว่า Gutter pool ทำงานยังไง</p>
<p>แม้ว่าการใช้ Gutter อาจจะเสี่ยงให้ user เห็นข้อมูลเก่าบ้าง แต่ backend ก็ปลอดภัยจาก overload<br>
และ Facebook ก็ถือว่าเป็น trade-off ที่ยอมรับได้ เพื่อแลกกับ <strong>availability</strong></p>
<h2 id="region-level-challenges">Region Level Challenges</h2>
<p>ในระดับ region มีหลาย frontend cluster ที่ต้องจัดการ และหนึ่งในความท้าทายหลักคือเรื่องการทำ <strong>Memcache invalidation</strong> ให้ครอบคลุมทุก cluster</p>
<p>เวลาผู้ใช้ request ข้อมูล พวกเขาอาจถูกส่งไปที่ frontend cluster คนละอัน ขึ้นอยู่กับ load balancer<br>
ซึ่งผลลัพธ์คือ ข้อมูลชุดเดียวกันอาจถูก <strong>cache ซ้ำ</strong> อยู่ในหลาย ๆ cluster ภายใน region เดียวกัน</p>
<p>พูดง่าย ๆ ก็คือ key เดียวกัน อาจถูก cache อยู่ใน Memcached server หลายชุดใน region นั้น<br>
ดูจากไดอะแกรม (ในต้นฉบับ) จะเห็นภาพชัดขึ้น</p>
<p><img src="/2025/05/56e9844c-451f-43d8-b713-46287ee3d94f.webp" alt=""></p>
<p>เช่น key “abc” กับ “xyz” ถูก cache อยู่ในหลาย frontend cluster ใน region เดียวกัน<br>
ถ้า key พวกนี้มีการ update ก็ต้องมีการ <strong>invalidate พร้อมกันทุก cluster</strong></p>
<h3 id="cluster-level-invalidation">Cluster Level Invalidation</h3>
<p>การ invalidate ข้อมูลในระดับ cluster ทำได้ง่ายกว่าเยอะ<br>
web server ที่แก้ไขข้อมูลจะเป็นคนจัดการ invalidate cache ใน cluster นั้นเองเลย</p>
<p>วิธีนี้ทำให้เกิดสิ่งที่เรียกว่า <strong>read-after-write consistency</strong> — คือผู้ใช้ที่เพิ่ง update ข้อมูลไป พอกดโหลดใหม่ ก็จะเห็นข้อมูลใหม่ทันที<br>
และยังช่วยลดเวลาที่ข้อมูล stale อยู่ใน cache ด้วย</p>
<p><em>Note</em>: <strong>read-after-write consistency</strong> หมายถึงการที่ user เห็นข้อมูลที่ตัวเองเพิ่งเปลี่ยนแปลงหลังจาก reload โดยไม่ต้องรอ</p>
<h3 id="region-level-invalidation">Region Level Invalidation</h3>
<p>แต่ถ้าเป็นระดับ region การ invalidate จะซับซ้อนขึ้นหน่อย และ webserver จะไม่รับหน้าที่นี้โดยตรง</p>
<p>Facebook เลยสร้างระบบที่เรียกว่า <strong>invalidation pipeline</strong> ขึ้นมา ซึ่งทำงานแบบนี้:</p>
<ul>
<li>มี daemon ตัวหนึ่งชื่อว่า <strong>mcsqueal</strong> ทำงานอยู่ในทุก database server ใน storage cluster</li>
<li>ตัว mcsqueal จะอ่าน <strong>commit log</strong> แล้วดูว่ามี delete อะไรเกิดขึ้นบ้าง แล้ว broadcast คำสั่งพวกนั้นออกไปให้ Memcache ทุก cluster ใน region</li>
<li>เพื่อให้เร็วขึ้น mcsqueal จะ batch คำสั่ง delete หลาย ๆ อันรวมเป็น packet ใหญ่ ๆ แล้วส่งไปให้ server ที่รัน <strong>mcrouter</strong></li>
<li>mcrouter จะ iterate ผ่าน delete แต่ละอันใน batch แล้ว route ไปยัง Memcache server ที่เหมาะสม</li>
</ul>
<p>ดูไดอะแกรม (ในต้นฉบับ) จะช่วยให้เข้าใจ flow นี้มากขึ้น</p>
<p><img src="/2025/05/d18d21ac-1bb4-4f54-bbf1-244578e7e7ad.webp" alt=""></p>
<h2 id="challenges-with-global-regions">Challenges with Global Regions</h2>
<p>ในระดับ Facebook ที่ต้องรองรับผู้ใช้ทั่วโลก ระบบก็ต้องมี data center หลายที่กระจายไปทั่วโลก</p>
<p>แต่การขยายไปหลาย region แบบนี้ก็มีปัญหาใหม่ตามมาด้วย โดยเฉพาะเรื่อง <strong>ความสอดคล้องของข้อมูล (consistency)</strong> ระหว่าง Memcache กับ persistent storage ใน region ต่าง ๆ</p>
<p>ในโครงสร้างของ Facebook, จะมี region หนึ่งเป็น <strong>primary database</strong><br>
ส่วน region อื่น ๆ จะเป็น <strong>read-only replica</strong> ซึ่ง sync ข้อมูลจาก primary ผ่านระบบ replication ของ MySQL</p>
<p>แต่พอมี replication เข้ามา ก็ต้องเจอกับสิ่งที่เรียกว่า <strong>replication lag</strong> คือ replica อาจตามข้อมูลของ primary ไม่ทัน</p>
<p>ซึ่งพอเป็นแบบนี้ ก็จะมี case ใหญ่ ๆ 2 แบบที่ต้องพิจารณาเกี่ยวกับเรื่อง <strong>consistency</strong></p>
<h2 id="writes-from-the-primary-region">Writes from the Primary Region</h2>
<p>สมมุติว่า web server ใน primary region (เช่น US) ได้รับ request จากผู้ใช้ให้เปลี่ยนรูปโปรไฟล์</p>
<p>เพื่อให้ข้อมูล consistent การเปลี่ยนแปลงนี้ต้องถูก <strong>propagate</strong> ไปยัง region อื่นด้วย<br>
นั่นหมายถึง replica databases ก็ต้อง ** update **<br>
และ Memcache ใน secondary region ทั้งหลายก็ต้อง <strong>invalidate</strong></p>
<p>จุดที่ยากคือการจัดการ <strong>invalidations ให้ sync กับ replication</strong></p>
<p>ถ้าเกิดว่า invalidation มาถึง secondary region (เช่น Europe) <strong>เร็วกว่าที่ replication  update เสร็จ</strong> อาจทำให้เกิด race condition แบบนี้:</p>
<ol>
<li>ผู้ใช้ที่อยู่ Europe พยายามดูรูปโปรไฟล์</li>
<li>ระบบไปดึงข้อมูลจาก cache — แต่โดน invalidate ไปแล้ว</li>
<li>เลยไปดึงจาก replica database ที่ยังไม่ sync — ได้รูปเก่า</li>
<li>ระบบเอารูปเก่านั้นกลับมาเซตใน cache อีกครั้ง</li>
<li>ถึงแม้ replication จะเสร็จในภายหลัง แต่ cache ดันเก็บรูปเก่าไว้แล้ว</li>
<li>ผลคือ request ต่อ ๆ ไปจะยังได้รูปเก่าจาก cache</li>
</ol>
<p>ในไดอะแกรม (ต้นฉบับ) จะอธิบายสถานการณ์นี้ให้ชัดเจน</p>
<p><img src="/2025/05/aadebcb8-f2cf-4c33-9479-6b01b4d4fc68.webp" alt=""></p>
<p>เพื่อป้องกัน race condition แบบนี้ Facebook ใช้แนวทางให้ <strong>storage cluster ที่ข้อมูลใหม่สุดเป็นคนส่ง invalidation</strong> ภายใน region โดยใช้ระบบ <strong>mcsqueal</strong> เหมือนที่เล่าไปก่อนหน้านี้</p>
<p>แนวทางนี้ช่วยให้มั่นใจว่า invalidation จะไม่ถูกส่งไปยัง replica region <strong>ก่อน</strong> ที่ข้อมูลจะ sync เสร็จ</p>
<h2 id="writes-from-the-non-primary-region">Writes from the Non-Primary Region</h2>
<p>เวลาเขียนข้อมูล (write) จาก region ที่ <strong>ไม่ใช่ primary</strong>, ลำดับของเหตุการณ์จะเป็นแบบนี้:</p>
<ul>
<li>ผู้ใช้ใน region รอง (secondary) เช่น Europe เปลี่ยนรูปโปรไฟล์</li>
<li>ถึงแม้ว่า reads จะอ่านจาก replica ได้ แต่ <strong>writes จะถูกส่งไป primary region</strong></li>
<li>พอ write สำเร็จ ข้อมูลต้องถูก replicate กลับมายัง region รอง</li>
<li>ระหว่างที่ replication ยังไม่ทัน บางที read request ที่วิ่งมาที่ region รอง อาจเจอ cache miss แล้วไปดึงข้อมูลเก่าจาก database มา cache ไว้ ซึ่งไม่ update</li>
</ul>
<p>Facebook แก้ปัญหานี้ด้วยการใช้แนวคิดชื่อว่า <strong>remote marker</strong></p>
<h3 id="remote-marker-คืออะไร">Remote Marker คืออะไร</h3>
<p>เป็นตัวบอกว่า ข้อมูลใน replica region อาจจะยัง **ไม่ update ** ถ้ามี marker นี้อยู่ — ให้ระบบไปดึงจาก primary แทน</p>
<h3 id="วิธีทำงาน">วิธีทำงาน:</h3>
<ol>
<li>เมื่อ client ส่ง request เพื่อ update  key <code>K</code></li>
<li>มันจะตั้ง <strong>remote marker <code>R</code></strong> สำหรับ key <code>K</code> ไว้ใน replica region</li>
<li>จากนั้นก็ส่ง write ไปที่ primary region</li>
<li>พร้อมกับลบ key <code>K</code> ออกจาก Memcache ใน region นั้น</li>
<li>พอมี read request เข้ามา มันจะเจอ cache miss</li>
<li>ระบบเช็คว่า marker <code>R</code> มีอยู่มั้ย — ถ้ามี ก็ส่ง query ไปที่ primary region แทน</li>
</ol>
<p>ในไดอะแกรม (ต้นฉบับ) จะเห็นภาพทุก step ชัดเจน</p>
<p>อาจมีบางคนคิดว่า แบบนี้ดู <strong>ช้าเกินไป</strong> เพราะต้องเช็ค cache → เช็ค marker → ค่อยยิงไป primary<br>
แต่ Facebook ยอมแลก <strong>latency เพิ่มขึ้น</strong> เพื่อให้โอกาสที่จะอ่านข้อมูล stale <strong>ลดลงอย่างมาก</strong></p>
<h2 id="single-server-optimizations">Single Server Optimizations</h2>
<p>จากที่เห็น Facebook ลงทุนกับสถาปัตยกรรมใหญ่ ๆ เยอะมากเพื่อ scale Memcached<br>
แต่ในขณะเดียวกัน พวกเขาก็ไม่มองข้ามการ <strong>optimize performance ของ Memcache server แบบรายตัว</strong> ด้วย</p>
<p>ถึงแม้การปรับพวกนี้ดูเล็ก ๆ แต่พอเอามาคูณกับ scale ของ Facebook แล้ว มัน impact ใหญ่มาก</p>
<h3 id="automatic-hash-table-expansion">Automatic Hash Table Expansion</h3>
<p>พอมี item ใน hash table เยอะขึ้น ความเร็วในการค้นหาอาจลดลงจาก O(1) ไปเป็น O(n) ถ้า table ยังมีขนาดเท่าเดิม<br>
นั่นแปลว่า performance จะค่อย ๆ แย่ลง</p>
<p>Facebook เลยเพิ่มระบบ <strong>ขยาย hash table อัตโนมัติ</strong><br>
พอจำนวน item เกิน threshold ที่กำหนด ระบบจะขยายขนาด table เป็น 2 เท่า เพื่อให้ lookup ยังคงเร็วเหมือนเดิมแม้ข้อมูลจะโตขึ้นเรื่อย ๆ</p>
<h2 id="multi-threaded-server-architecture">Multi-Threaded Server Architecture</h2>
<p>การรับ request จำนวนมากบน thread เดียว อาจทำให้ latency เพิ่มขึ้นและ throughput ลดลง</p>
<p>Facebook เลยปรับ Memcache server ให้ <strong>รองรับ multi-thread</strong> เพื่อสามารถจัดการ request หลาย ๆ ตัวพร้อมกันได้แบบ concurrent</p>
<h3 id="dedicated-udp-port-for-each-thread">Dedicated UDP Port for Each Thread</h3>
<p>ถ้า thread หลายตัว share UDP port เดียวกัน จะเกิด <strong>contention</strong> (แย่งกันใช้) ซึ่งส่งผลให้ performance แย่ลง</p>
<p>Facebook เลยเพิ่มระบบให้แต่ละ thread ใช้ <strong>UDP port แยกกันคนละอัน</strong> ทำให้แต่ละ thread ทำงานได้เต็มที่ ไม่มีแย่งกัน</p>
<h3 id="adaptive-slab-allocator">Adaptive Slab Allocator</h3>
<p>ถ้าการจัดการ memory ทำได้ไม่ดี อาจเกิด fragmentation หรือใช้ resource ได้ไม่คุ้ม</p>
<p>Facebook เลยใช้เทคนิคที่เรียกว่า <strong>Adaptive Slab Allocator</strong> เพื่อ optimize การใช้ memory ในแต่ละ Memcache server</p>
<p>โดย allocator จะแบ่ง memory เป็นก้อนใหญ่ ๆ เรียกว่า <strong>slab</strong> ซึ่ง slab แต่ละก้อนจะถูกแบ่งย่อยออกมาอีกเป็น block ที่มีขนาดคงที่</p>
<p>ที่พิเศษคือ <strong>ขนาดของ slab จะถูกปรับแบบ dynamic</strong> ตาม pattern ของ request ที่ระบบเจอ เพื่อให้ใช้ memory ได้คุ้มค่าที่สุด</p>
<h2 id="conclusion">Conclusion</h2>
<p>การเดินทางของ Facebook ในการ scale Memcached เป็น case study ที่ดีมากสำหรับนักพัฒนาและ engineers ทุกคน<br>
มันโชว์ให้เห็นถึงความท้าทายของการสร้าง social network ที่กระจายอยู่ทั่วโลก ต้องรองรับข้อมูลมหาศาล และ user เป็น billions คน</p>
<p>จากการออกแบบและ optimize Memcache ของ Facebook เราจะเห็นเลยว่า ถ้าจะรับมือกับปัญหา <strong>scalability</strong> จริง ๆ<br>
ต้องลงมือแก้ทั้งในระดับสถาปัตยกรรมภาพรวม และระดับลึก ๆ ใน server ทุกจุดมีผลต่อ performance และ reliability ของระบบทั้งหมด</p>
<ol>
<li>การยอมรับ <strong>eventual consistency</strong> คือหัวใจของการสร้างระบบที่เร็วและ available<br>
แต่อย่าลืมว่า ทุกการตัดสินใจต้องเข้าใจ <strong>trade-off</strong> ให้ดี</li>
<li><strong>Failure เป็นเรื่องที่เลี่ยงไม่ได้</strong> ต้องออกแบบระบบให้รับมือกับมันได้</li>
<li>การ optimize ทำได้หลายระดับ — ตั้งแต่ระบบใหญ่ยัน detail เล็ก ๆ ใน server</li>
</ol>
<h4 id="references">References:</h4>
<ul>
<li><a href="https://blog.bytebytego.com/p/how-facebook-served-billions-of-requests">blog.bytebytego.com</a></li>
<li><a href="https://research.facebook.com/publications/scaling-memcache-at-facebook/">facebook.com</a></li>
</ul>  </div> </article> </main> <footer data-astro-cid-sz7xmlte>
&copy; 2025 BigJoe. All rights reserved.
<div class="social-links" data-astro-cid-sz7xmlte> <a href="https://www.facebook.com/profile.php?id=61572916280350" target="_blank" data-astro-cid-sz7xmlte> <svg xmlns="http://www.w3.org/2000/svg" width="40" height="40" viewBox="0 0 24 24" data-astro-cid-sz7xmlte> <path fill="currentColor" d="M13.458 21.696c4.693-.704 8.292-4.753 8.292-9.642c0-5.385-4.365-9.75-9.75-9.75s-9.75 4.365-9.75 9.75c0 4.89 3.599 8.938 8.292 9.642v-6.798h-2.05a.486.486 0 0 1-.486-.486v-1.843c0-.269.218-.486.486-.486h2.05l-.072-1.943c0-.942.175-2.471 1.342-3.307c.816-.583 1.423-.693 2.397-.693c.845 0 1.426.084 1.81.14l.188.025a.193.193 0 0 1 .168.192v2.04c0 .113-.095.2-.205.194h-.038c-.114.004-.71.029-1.216.029c-.89 0-1.458.406-1.458 1.755v1.568h2.192c.3 0 .529.27.48.566l-.28 1.843a.486.486 0 0 1-.479.406h-1.913z" data-astro-cid-sz7xmlte></path> </svg> </a> <a href="https://x.com/joewalker_xyz" target="_blank" data-astro-cid-sz7xmlte> <svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24" data-astro-cid-sz7xmlte> <g fill="none" fill-rule="evenodd" data-astro-cid-sz7xmlte> <path d="m12.594 23.258l-.012.002l-.071.035l-.02.004l-.014-.004l-.071-.036q-.016-.004-.024.006l-.004.01l-.017.428l.005.02l.01.013l.104.074l.015.004l.012-.004l.104-.074l.012-.016l.004-.017l-.017-.427q-.004-.016-.016-.018m.264-.113l-.014.002l-.184.093l-.01.01l-.003.011l.018.43l.005.012l.008.008l.201.092q.019.005.029-.008l.004-.014l-.034-.614q-.005-.019-.02-.022m-.715.002a.02.02 0 0 0-.027.006l-.006.014l-.034.614q.001.018.017.024l.015-.002l.201-.093l.01-.008l.003-.011l.018-.43l-.003-.012l-.01-.01z" data-astro-cid-sz7xmlte></path> <path fill="currentColor" d="M19.753 4.659a1 1 0 0 0-1.506-1.317l-5.11 5.84L8.8 3.4A1 1 0 0 0 8 3H4a1 1 0 0 0-.8 1.6l6.437 8.582l-5.39 6.16a1 1 0 0 0 1.506 1.317l5.11-5.841L15.2 20.6a1 1 0 0 0 .8.4h4a1 1 0 0 0 .8-1.6l-6.437-8.582l5.39-6.16ZM16.5 19L6 5h1.5L18 19z" data-astro-cid-sz7xmlte></path> </g> </svg> </a> <a href="https://github.com/joeteerawit" target="_blank" data-astro-cid-sz7xmlte> <svg viewBox="0 0 16 16" aria-hidden="true" width="32" height="32" data-astro-cid-sz7xmlte><path fill="currentColor" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8z" data-astro-cid-sz7xmlte></path></svg> </a> </div> </footer>  <script type="module">const l=window.innerWidth<=720;document.addEventListener("DOMContentLoaded",()=>{const r=t=>{let e=t.parentElement;for(;e;){if(e.tagName==="NAV")return!0;e=e.parentElement}return!1};document.querySelectorAll(".prose p").forEach(t=>{r(t)||t.querySelector("img")&&(t.style.justifyItems="center")}),document.querySelectorAll("h1, h2, h3, h4, h5, h6").forEach(t=>{const e=t;if(r(t))return;const o=t.nextElementSibling,n=o&&o.tagName.toLowerCase()==="pre";l?(e.style.marginTop="1rem",e.style.marginBottom=n?"0":"1rem"):(e.style.marginTop="2rem",e.style.marginBottom=n?"0":"2rem")}),document.querySelectorAll(".diff.remove").forEach(t=>{if(r(t))return;const e=document.createElement("span");e.className="diff-indicator",e.textContent="-",e.style.display="inline-block",e.style.width="15px",e.style.color="#d73a49",e.style.fontWeight="bold",e.style.userSelect="none",t.insertBefore(e,t.firstChild)}),document.querySelectorAll(".diff.add").forEach(t=>{if(r(t))return;const e=document.createElement("span");e.className="diff-indicator",e.textContent="+",e.style.display="inline-block",e.style.width="15px",e.style.color="#22863a",e.style.fontWeight="bold",e.style.userSelect="none",t.insertBefore(e,t.firstChild)}),document.querySelectorAll('a[href^="http"]').forEach(t=>{r(t)||(t.setAttribute("target","_blank"),t.setAttribute("rel","noopener noreferrer"))})});</script> </body></html>